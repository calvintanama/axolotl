base_model: "???" #modified model (pruned, inserted extra module etc)
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
tokenizer_config: "???" #tokenizer for the modified model
trust_remote_code: true

load_in_8bit: false
load_in_4bit: false
strict: false
#bnb_config_kwargs:
#  bnb_4bit_quant_storage: uint8

pretraining_dataset:
  - path: "HuggingFaceFW/fineweb"
    name: "sample-10BT"
    split: "train"
    type: pretrain
    text_column: "text"
pretrain_multipack_buffer_size: 100000
shuffle_merged_datasets: true
dataset_processes:
val_set_size: 0
output_dir: "???" #output directory to save pretrained model
chat_template:

sequence_len: 4096
sample_packing: true
eval_sample_packing: false
pad_to_sequence_len: true
pretrain_multipack_attn: true

overrides_of_model_config:
  use_cache: false

adapter:

hf_mlflow_log_artifacts: false
use_mlflow: true

gradient_accumulation_steps: 1
micro_batch_size: 24
num_epochs: 1
optimizer: adamw_hf #during healing use normal adamw as we only healing the auxilary module
lr_scheduler: cosine
learning_rate: 0.00005

train_on_inputs: false
group_by_length: false
bf16: auto
fp16: false
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention: false
sdp_attention: false
flash_attention: true

loss_watchdog_threshold: 12
loss_watchdog_patience: 3

max_steps: 5000
#warmup_ratio: 0.1
warmup_steps: 200
#eval_steps: 100
#eval_table_size:
#eval_max_new_tokens: 128
save_steps: 500
save_total_limit: 3
debug:
deepspeed:
weight_decay: 0.0001
fsdp:
fsdp_config:
special_tokens:
eval_batch_size: 8
save_safetensors: true

training_mode: "kd_l" # or "kd" for normal knowledge distillation
teacher_model: "???" # insert path to teacher model
only_train_extra_module: true
trainable_decoder_indices:
prune_start_index: 1
prune_end_index: 16
temperature: 0.5
alpha: 0.5
beta: 0.25
gamma: 0.25
